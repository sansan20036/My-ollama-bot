import shutil
import os
import logging
import json
import gc
import pandas as pd
from typing import List, Optional  # ğŸŸ¢ è£œä¸Š Optional
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse, StreamingResponse
from pydantic import BaseModel
import asyncio
import random
import requests

# ç¢ºä¿ file_factory å­˜åœ¨
from file_factory import FileLoaderFactory

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import CrossEncoder
# ğŸŸ¢ æ–°å¢ï¼šç”¨æ–¼å»ºæ§‹å¤šæ¨¡æ…‹è¨Šæ¯ (è™•ç†åœ–ç‰‡çš„é—œéµ)
from langchain_core.messages import HumanMessage

# ğŸŸ¢ Import å€å¡Š
from langchain_community.retrievers import BM25Retriever

# ğŸŸ¢ æ‰‹å‹•å®šç¾© EnsembleRetriever (Polyfill)
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun


class EnsembleRetriever(BaseRetriever):
    """
    è‡ªå®šç¾©çš„æ··åˆæª¢ç´¢å™¨ (Polyfill)
    """
    retrievers: List[BaseRetriever]
    weights: List[float]

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:

        all_docs = []
        seen_contents = set()

        for retriever in self.retrievers:
            docs = retriever.invoke(query)
            for doc in docs:
                if doc.page_content not in seen_contents:
                    # åœ¨ metadata åŠ å…¥ã€Œæ¨™è¨»ç‹€æ…‹ã€ï¼Œé˜²æ­¢é‡è¤‡å¼•ç”¨
                    doc.metadata["already_cited"] = False
                    all_docs.append(doc)
                    seen_contents.add(doc.page_content)
        return all_docs[:10]


# å¼•å…¥ Agent
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent

# è¨­å®š Log
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


def get_optimized_history(history, max_turns=10):
    """
    åšå£«è«–æ–‡äº®é»ï¼šå‹•æ…‹æ»‘å‹•çª—å£èˆ‡è³‡è¨Šå£“ç¸®
    1. é™åˆ¶å°è©±è¼ªæ•¸é˜²æ­¢ Token æº¢ä½
    2. æˆªæ–·éé•·çš„å…§å®¹ï¼ˆå¦‚å…ˆå‰çš„è¡¨æ ¼æ•¸æ“šï¼‰ä¿ç•™èªæ„æ ¸å¿ƒ
    """
    recent_history = history[-max_turns:]
    history_lines = []
    for msg in recent_history:
        role = "ä½¿ç”¨è€…" if msg['role'] == "User" else "åŠ©ç†"
        # ğŸŸ¢ é—œéµå„ªåŒ–ï¼šå¦‚æœå…§å®¹è¶…é 200 å­—ï¼ˆé€šå¸¸æ˜¯ Agent ç”¢ç”Ÿçš„å ±è¡¨ï¼‰ï¼Œå‰‡é€²è¡Œæ‘˜è¦æˆªæ–·
        content = (msg['content'][:150] + " [å¾ŒçºŒå…§å®¹å·²çœç•¥...]") if len(msg['content']) > 200 else msg['content']
        history_lines.append(f"{role}: {content}")
    return "\n".join(history_lines)


# ================= é…ç½®å€ =================
MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)

DEFAULT_MODEL = "gpt-oss:20b"

# åˆå§‹åŒ–è®Šæ•¸
vector_db = None
CHAT_HISTORY = []
GLOBAL_FILE_CONTENT = ""
GLOBAL_DFS = {}
GLOBAL_DOCS = []


def init_vector_db():
    global vector_db, GLOBAL_DOCS

    # ğŸŸ¢ 1. è¨­å®šè³‡æ–™åº«å„²å­˜è·¯å¾‘
    persist_dir = "./chroma_db"

    logger.info(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–è³‡æ–™åº«ï¼Œè·¯å¾‘: {persist_dir}")

    # ğŸŸ¢ 2. åˆå§‹åŒ– Chroma (æŒä¹…åŒ–)
    vector_db = Chroma(
        embedding_function=embeddings,
        persist_directory=persist_dir
    )

    # ğŸŸ¢ 3. é‡å»º BM25 ç´¢å¼•
    try:
        existing_data = vector_db.get()
        if existing_data and len(existing_data['ids']) > 0:
            print(f"ğŸ“¦ åµæ¸¬åˆ°æ­·å²å­˜æª”ï¼Œæ­£åœ¨é‡å»ºé—œéµå­—ç´¢å¼• (å…± {len(existing_data['ids'])} ç­†)...")

            GLOBAL_DOCS = []
            for i in range(len(existing_data['ids'])):
                doc = Document(
                    page_content=existing_data['documents'][i],
                    metadata=existing_data['metadatas'][i] if existing_data['metadatas'] else {}
                )
                GLOBAL_DOCS.append(doc)

            print(f"âœ… æˆåŠŸæ¢å¾©è¨˜æ†¶ï¼ç›®å‰æ“æœ‰ {len(GLOBAL_DOCS)} ç­†çŸ¥è­˜ç‰‡æ®µã€‚")
        else:
            print("âœ¨ å…¨æ–°é–‹å§‹ï¼šè³‡æ–™åº«ç›®å‰æ˜¯ç©ºçš„ã€‚")

    except Exception as e:
        print(f"âš ï¸ é‡å»ºç´¢å¼•æ™‚ç™¼ç”Ÿå°æ’æ›²: {e}")

    logger.info(f"âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")


init_vector_db()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)


class ChatRequest(BaseModel):
    query: str
    model_name: str = DEFAULT_MODEL
    # ğŸŸ¢ æ–°å¢ï¼šæ¥æ”¶åœ–ç‰‡ Base64 åˆ—è¡¨ (Optional)
    # æ²’æœ‰é€™è¡Œï¼Œå¾Œç«¯å°±æœƒå¿½ç•¥å‰ç«¯å‚³ä¾†çš„åœ–ç‰‡ï¼
    images: Optional[List[str]] = None


@app.get("/")
async def root():
    return RedirectResponse(url="/docs")


# ================= é‡ç½®é‚è¼¯ (Deep Clean) =================
@app.post("/reset")
async def reset_history():
    global CHAT_HISTORY, vector_db, GLOBAL_FILE_CONTENT, GLOBAL_DFS, GLOBAL_DOCS
    logger.info("ğŸ§¹ åŸ·è¡Œç³»çµ±é‡ç½® (Deep Clean)...")

    try:
        CHAT_HISTORY = []
        GLOBAL_FILE_CONTENT = ""
        GLOBAL_DFS = {}
        GLOBAL_DOCS = []

        persist_dir = "./chroma_db"

        if vector_db:
            try:
                ids = vector_db.get()['ids']
                if ids:
                    vector_db.delete(ids)
            except Exception as e:
                logger.warning(f"é‚è¼¯åˆªé™¤å¤±æ•—: {e}")

            vector_db = None
            gc.collect()  # ğŸŸ¢ å¼·åˆ¶å›æ”¶åƒåœ¾ï¼Œç¢ºä¿æª”æ¡ˆæŒ‡æ¨™è¢«é‡‹æ”¾
            await asyncio.sleep(1)  # ğŸŸ¢ çµ¦ Windows ä¸€ç§’é˜åæ‡‰æ™‚é–“

        if os.path.exists(persist_dir):
            try:
                await asyncio.sleep(1)
                shutil.rmtree(persist_dir, ignore_errors=True)
                logger.info("ğŸ—‘ï¸ å·²åˆªé™¤å¯¦é«”è³‡æ–™åº«æª”æ¡ˆ")
            except Exception as e:
                logger.error(f"âŒ ç‰©ç†åˆªé™¤å¤±æ•—: {e}")

        init_vector_db()
        return {"message": "ç³»çµ±å·²å®Œå…¨é‡ç½® (è¨˜æ†¶å·²æ¸…é™¤)"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ================= æ–°å¢ï¼šè»Ÿé‡ç½® (æ–°å°è©±ï¼Œä¿ç•™æª”æ¡ˆ) =================
@app.post("/new_chat")
async def start_new_chat():
    global CHAT_HISTORY
    logger.info("ğŸ§¹ åŸ·è¡Œæ–°å°è©±")

    # åªæ¸…ç©ºå°è©±ç´€éŒ„
    CHAT_HISTORY = []

    # å›å‚³ç›®å‰çš„æª”æ¡ˆæ•¸é‡ï¼Œè®“å‰ç«¯çŸ¥é“æª”æ¡ˆé‚„æ´»è‘—
    current_file_count = len(GLOBAL_DFS) + len(set([d.metadata.get("source") for d in GLOBAL_DOCS]))

    return {
        "message": "å°è©±ç´€éŒ„å·²æ¸…é™¤ (æª”æ¡ˆè¨˜æ†¶å·²ä¿ç•™)",
        "status": "success",
        "kept_files": current_file_count
    }

# ================= ç²å–æ¨¡å‹æ¸…å–® API =================
@app.get("/models")
async def get_models():
    try:
        ollama_api_url = "http://git.tedpc.com.tw:11434/api/tags"
        response = requests.get(ollama_api_url, timeout=5)
        response.raise_for_status()
        data = response.json()
        models = [model["name"] for model in data.get("models", [])]
        return {"models": models}
    except Exception as e:
        logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹æ¸…å–®: {e}")
        return {"models": ["gpt-oss:20b", "llama3.1:latest"]}


# ================= ä¸Šå‚³é‚è¼¯ =================
@app.post("/upload")
async def upload_files(files: List[UploadFile] = File(...)):
    # ğŸŸ¢ è¨»è§£æ‰é‡ç½®ï¼Œå…è¨±ç´¯åŠ æª”æ¡ˆ
    # await reset_history()

    global vector_db, GLOBAL_FILE_CONTENT, GLOBAL_DFS, GLOBAL_DOCS

    try:
        processed_files = []
        full_text_list = []
        temp_dfs = []

        for file in files:
            temp_filename = f"temp_{file.filename}"
            with open(temp_filename, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)

            if file.filename.endswith(('.xlsx', '.xls')):
                try:
                    # ğŸŸ¢ ä½¿ç”¨ with ç¢ºä¿ excel_file åœ¨è®€å–å®Œå¾Œè‡ªå‹•é—œé–‰é‡‹æ”¾æª”æ¡ˆé–
                    with pd.ExcelFile(temp_filename) as excel_file:
                        for sheet_name in excel_file.sheet_names:
                            df = pd.read_excel(excel_file, sheet_name=sheet_name)
                            if not df.empty:
                                display_name = f"{file.filename} ({sheet_name})"
                                temp_dfs.append((display_name, df))
                except Exception as e:
                    logger.error(f"è®€å– Excel å¤±æ•—: {e}")

            elif file.filename.endswith('.csv'):
                try:
                    df = pd.read_csv(temp_filename)
                    if not df.empty:
                        temp_dfs.append((file.filename, df))  # ğŸŸ¢ å­˜æˆ tuple
                except Exception as e:
                    logger.error(f"è®€å– CSV å¤±æ•—: {e}")

            try:
                loader = FileLoaderFactory.get_loader(temp_filename, file.filename)
                raw_text = loader.extract_text()
                if raw_text:
                    full_text_list.append(f"ã€æª”æ¡ˆ: {file.filename}ã€‘\n{raw_text}\n")

                    chunks = text_splitter.split_text(raw_text)
                    docs = [Document(page_content=c, metadata={"source": file.filename}) for c in chunks]
                    if docs:
                        vector_db.add_documents(docs)
                        GLOBAL_DOCS.extend(docs)
                        processed_files.append(file.filename)
            finally:
                if os.path.exists(temp_filename):
                    os.remove(temp_filename)

        # ğŸŸ¢ ä¿®å¾©ï¼šä½¿ç”¨ extend ç´¯ç©è³‡æ–™è¡¨ï¼Œè€Œä¸æ˜¯è¦†è“‹
        if temp_dfs:
            for file_name, df in temp_dfs:
                GLOBAL_DFS[file_name] = df  # é€™æ˜¯å­—å…¸è³¦å€¼ï¼Œæ­£ç¢º

        combined_text = "\n".join(full_text_list)

        # ğŸŸ¢ ä¿®å¾©ï¼šç´¯ç©ç´”æ–‡å­—å…§å®¹
        if len(GLOBAL_FILE_CONTENT) + len(combined_text) < 10000:
            GLOBAL_FILE_CONTENT += "\n" + combined_text
        else:
            pass

        has_dfs = len(GLOBAL_DFS) > 0

        mode = "RAG_MODE"
        if has_dfs:
            mode = "PANDAS_AGENT"
        elif GLOBAL_FILE_CONTENT:
            mode = "GOD_MODE"

        return {
            "status": "success",
            "processed_files": processed_files,
            "current_inventory": list(GLOBAL_DFS.keys()),
            "mode": mode
        }

    except Exception as e:
        logger.error(f"ä¸Šå‚³å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ================= èŠå¤©é‚è¼¯ (å®Œæ•´ç‰ˆï¼šå¤šæ¨¡æ…‹ + æ··åˆæª¢ç´¢) =================
# ================= èŠå¤©é‚è¼¯ (ä¿®æ­£ç‰ˆï¼šè¦–è¦º + æª”æ¡ˆè¨˜æ†¶å…±å­˜) =================
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    global CHAT_HISTORY, GLOBAL_FILE_CONTENT, GLOBAL_DFS, GLOBAL_DOCS

    # 1. æº–å‚™åŸºç¤è³‡è¨Š
    raw_sources = [doc.metadata.get("source", "") for doc in GLOBAL_DOCS] + list(GLOBAL_DFS.keys())
    clean_sources = []
    for s in raw_sources:
        if not s: continue
        name = s.replace("temp_", "")
        name = name.split(" (")[0] if " (" in name else name
        clean_sources.append(name)
    unique_files_list = list(set(clean_sources))
    total_file_count = len(unique_files_list)

    current_images = len(request.images) if request.images else 0
    logger.info(f"ğŸ—£ï¸ æ”¶åˆ°å•é¡Œ: {request.query} (åœ–ç‰‡: {current_images}, æª”æ¡ˆåº«å­˜: {total_file_count})")

    try:
        model_to_use = request.model_name if request.model_name else DEFAULT_MODEL

        llm = ChatOllama(
            model=model_to_use,
            temperature=0.2,
            base_url="http://git.tedpc.com.tw:11434",
            request_timeout=900.0,
            num_thread=8,
            num_predict=2048,
            headers={"X-Accel-Buffering": "no", "Cache-Control": "no-cache"}
        )

        history_str = get_optimized_history(CHAT_HISTORY, max_turns=5)

        async def generate_response():
            # ğŸŸ¢ [ä¿®æ­£é—œéµ 1]ï¼šå…ˆæŠŠæª”æ¡ˆæ‘˜è¦ (Context) æº–å‚™å¥½ï¼Œä¸ç®¡æœ‰æ²’æœ‰åœ–éƒ½è¦ç”¨
            context_summary_list = []

            # 1. æª”æ¡ˆæ¸…å–®
            if unique_files_list:
                context_summary_list.append(f"å·²è¼‰å…¥æª”æ¡ˆæ¸…å–®ï¼š{', '.join(unique_files_list)}")
            else:
                context_summary_list.append("ç›®å‰ç„¡è¼‰å…¥æª”æ¡ˆ")

            # 2. Excel/CSV çµæ§‹æ‘˜è¦
            if len(GLOBAL_DFS) > 0:
                context_summary_list.append("\nã€è¡¨æ ¼æª”æ¡ˆæ‘˜è¦è³‡è¨Šã€‘(AIè«‹æ³¨æ„æ­¤å€å¡Š)ï¼š")
                for display_name, df in GLOBAL_DFS.items():
                    clean_name = display_name.replace("temp_", "")
                    columns = ", ".join(list(df.columns))
                    if len(df) <= 20:
                        preview_data = df.to_markdown(index=False)
                    else:
                        preview_data = df.head(3).to_markdown(index=False) + "\n(åªé¡¯ç¤ºå‰ 3 ç­†...)"

                    summary = (
                        f"- æª”æ¡ˆ `{clean_name}`ï¼š\n"
                        f"  - é¡å‹ï¼šçµæ§‹åŒ–æ•¸æ“šè¡¨\n"
                        f"  - è³‡æ–™ç­†æ•¸ï¼š{len(df)} ç­†\n"
                        f"  - åŒ…å«æ¬„ä½ï¼š[{columns}]\n"
                        f"  - è³‡æ–™é è¦½ï¼š\n{preview_data}\n"
                    )
                    context_summary_list.append(summary)

            # 3. RAG æª¢ç´¢ (å¦‚æœæ˜¯æ··åˆæ¨¡å¼ï¼Œä¹Ÿè¦å˜—è©¦æ’ˆä¸€é»æ–‡å­—è³‡æ–™)
            #    (é€™è£¡åšä¸€å€‹è¼•é‡ç´šæª¢ç´¢ï¼Œç¢ºä¿å¦‚æœå•æ–‡å­—æª”ä¹Ÿèƒ½å›ç­”)
            rag_context = ""
            if len(GLOBAL_DOCS) > 0 and vector_db:
                try:
                    # ç°¡å–®ç”¨ BM25 æ’ˆå‰ 3 ç­†ç›¸é—œçš„ï¼Œä½œç‚ºèƒŒæ™¯çŸ¥è­˜
                    bm25_retriever = BM25Retriever.from_documents(GLOBAL_DOCS)
                    bm25_retriever.k = 3
                    docs = bm25_retriever.invoke(request.query)
                    if docs:
                        rag_context = "\nã€ç›¸é—œæ–‡å­—æª”æ¡ˆç‰‡æ®µã€‘:\n" + "\n".join([d.page_content[:200] for d in docs])
                        context_summary_list.append(rag_context)
                except Exception:
                    pass

            # çµ„åˆæœ€çµ‚çš„ System Context
            files_context = "\n".join(context_summary_list)

            # ğŸŸ¢ [ä¿®æ­£é—œéµ 2]: è¦–è¦ºæ¨¡å¼ (Vision Mode) ç¾åœ¨ä¹ŸåŒ…å« files_context äº†
            if request.images and len(request.images) > 0:
                # çµ„åˆ Promptï¼šåŒ…å« ä½¿ç”¨è€…å•é¡Œ + æª”æ¡ˆèƒŒæ™¯çŸ¥è­˜ + æ­·å²å°è©±
                # ğŸŸ¢ ä¿®æ­£ï¼šå¼·åˆ¶ AI å…ˆæè¿°åœ–ç‰‡ï¼Œå†é—œè¯æª”æ¡ˆï¼Œé˜²æ­¢çæ°
                system_instruction = """ç³»çµ±æŒ‡ä»¤ï¼šä½ æ˜¯ä¸€å€‹å…·å‚™è¦–è¦ºèƒ½åŠ›çš„æ•¸æ“šåˆ†æå¸«ã€‚

                ä½¿ç”¨è€…æ­£åœ¨å±•ç¤ºä¸€å¼µåœ–ç‰‡ä¸¦è©¢å•å•é¡Œã€‚

                âš ï¸ ä½ çš„æ€è€ƒæ­¥é©Ÿ (å¿…é ˆåš´æ ¼éµå®ˆ)ï¼š
                1. **è¦–è¦ºæª¢æ¸¬**ï¼šé¦–å…ˆï¼Œè«‹å®¢è§€ã€èª å¯¦åœ°æè¿°ä½ ã€ŒçœŸæ­£ã€åœ¨åœ–ç‰‡ä¸­çœ‹åˆ°äº†ä»€éº¼ã€‚å¦‚æœåœ–ç‰‡ä¸­æ²’æœ‰é¡¯ç¤ºå¡ï¼Œè«‹ç›´æ¥èªªå‡ºä¾†ï¼Œä¸è¦çæ°ã€‚
                2. **é—œè¯æ€§åˆ†æ**ï¼šæ¥è‘—ï¼Œå°‡ä½ çœ‹åˆ°çš„å…§å®¹èˆ‡ [èƒŒæ™¯çŸ¥è­˜] é€²è¡Œæ¯”å°ã€‚
                3. **å›ç­”å•é¡Œ**ï¼šæ ¹æ“šæ¯”å°çµæœå›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
                """

                # 2. å®‰å…¨æ‹¼æ¥ (Safe Concatenation)
                mixed_prompt = (
                        system_instruction +
                        "\n\nã€èƒŒæ™¯çŸ¥è­˜/æª”æ¡ˆå…§å®¹ã€‘ï¼š\n" + files_context +
                        "\n\nã€æ­·å²å°è©±ã€‘ï¼š\n" + history_str +
                        "\n\nã€ä½¿ç”¨è€…ç›®å‰å•é¡Œã€‘ï¼š\n" + request.query +
                        "\n\nè«‹ç¶œåˆåœ–ç‰‡å…§å®¹èˆ‡ä¸Šè¿°èƒŒæ™¯çŸ¥è­˜å›ç­”ã€‚"
                )

                content_parts = [{"type": "text", "text": mixed_prompt}]
                for img_base64 in request.images:
                    content_parts.append({
                        "type": "image_url",
                        "image_url": f"data:image/jpeg;base64,{img_base64}"
                    })

                message = HumanMessage(content=content_parts)
                async for chunk in llm.astream([message]):
                    yield chunk.content

                CHAT_HISTORY.append({"role": "User", "content": f"[åœ–ç‰‡] {request.query}"})
                # æ³¨æ„ï¼šé€™è£¡ä¾ç„¶ Returnï¼Œå› ç‚ºè¦–è¦ºæ¨¡å‹ä¸é©åˆè·‘ Pandas Agentï¼Œç›´æ¥å›ç­”å³å¯
                return

            # --- ä»¥ä¸‹ç¶­æŒåŸæœ¬çš„é‚è¼¯ (å¿«é€Ÿé€šé“ã€Agentã€RAG) ---

            # æª”æ¡ˆæ¸…å–®å¿«é€Ÿé€šé“
            trigger_keywords = ["å“ªäº›", "å“ªå¹¾ä»½", "åˆ†åˆ¥æ˜¯", "æª”æ¡ˆæ¸…å–®", "åˆ—å‡º", "æœ‰ä»€éº¼"]
            exclusion_keywords = ["å…§å®¹", "æ¬„ä½", "æ•¸æ“š", "è³‡æ–™", "é—œæ–¼", "æè¿°", "ç¿»è­¯", "è¨ˆç®—", "ç¸½çµ"]
            is_listing_query = any(k in request.query for k in trigger_keywords)
            has_exclusion = any(k in request.query for k in exclusion_keywords)

            if is_listing_query and not has_exclusion:
                fast_response = f"ç›®å‰ç³»çµ±ä¸­å·²è¼‰å…¥çš„ {len(unique_files_list)} ä»½æª”æ¡ˆå¦‚ä¸‹ï¼š\n\n"
                for i, f_name in enumerate(unique_files_list):
                    fast_response += f"{i + 1}. {f_name}\n"
                for char in fast_response:
                    yield char
                    await asyncio.sleep(0.005)
                CHAT_HISTORY.append({"role": "User", "content": request.query})
                CHAT_HISTORY.append({"role": "AI", "content": fast_response})
                return

            # Pandas Agent (æ•¸æ“šåˆ†æ)
            is_calc_query = any(k in request.query for k in ["ç®—", "å¹³å‡", "ç¸½å’Œ", "åŠ ç¸½", "å¹¾ä»½", "æª”æ¡ˆ", "è³‡æ–™"])
            if len(GLOBAL_DFS) > 0 and is_calc_query:
                try:
                    df_list = list(GLOBAL_DFS.values())
                    file_names = list(GLOBAL_DFS.keys())
                    inventory_str = "\n".join([f"df{i + 1}: {name}" for i, name in enumerate(file_names)])

                    # è‡¨æ™‚å»ºç«‹ temp=0 çš„ Agent å°ˆç”¨ LLM
                    agent_llm = ChatOllama(
                        model=model_to_use,
                        temperature=0.0,
                        base_url="http://git.tedpc.com.tw:11434",
                        request_timeout=900.0,
                        num_thread=8,
                        num_predict=2048,
                        headers={"X-Accel-Buffering": "no", "Cache-Control": "no-cache"}
                    )

                    custom_prefix = f"""ä½ ç¾åœ¨æ˜¯ä¸€å€‹å…·å‚™åš´æ ¼é‚è¼¯çš„æ•¸æ“šåˆ†æå®˜ã€‚
                                    âš ï¸ ç³»çµ±ç’°å¢ƒè³‡è¨Šï¼š
                                    - ä½ çš„ Python ç’°å¢ƒä¸­æœ‰ {len(df_list)} å€‹è³‡æ–™è¡¨ï¼š{inventory_str}ã€‚
                                    - ç¸½æª”æ¡ˆæ¸…å–®ï¼š[{files_context}]ã€‚
                                    ä½ çš„æ€è€ƒè¦å‰‡ (æ¥µé‡è¦)ï¼š
                                    1. å¿…é ˆä½¿ç”¨ ReAct æ¡†æ¶ï¼šThought -> Action -> Observation -> Final Answerã€‚
                                    2. **ç¦æ­¢**ä½¿ç”¨ JSON æ ¼å¼å›æ‡‰ï¼Œè«‹ä½¿ç”¨ç´”æ–‡å­—æ ¼å¼ã€‚
                                    3. Action å¿…é ˆæ˜¯ `python_repl_ast`ã€‚
                                    4. ç•¶ä½ å¾—å‡ºçµè«–å¾Œï¼Œ**å¿…é ˆ**ä»¥ 'Final Answer: ' é–‹é ­è¼¸å‡ºçµæœã€‚
                                    5. ç¦æ­¢ç›´æ¥å›è¦† "Answer:" æˆ– "ç­”æ¡ˆæ˜¯"ï¼Œé€™æœƒå°è‡´ç³»çµ±è§£æå¤±æ•—ã€‚
                                    6. å¦‚æœè¨ˆç®—å‡ºç¾éŒ¯èª¤ï¼Œè«‹é‡æ–°æª¢æŸ¥ DataFrame çš„æ¬„ä½åç¨±ã€‚
                                    """
                    agent = create_pandas_dataframe_agent(
                        agent_llm,
                        df_list,
                        verbose=True,
                        allow_dangerous_code=True,
                        agent_type="zero-shot-react-description",
                        prefix=custom_prefix
                    )
                    result = await asyncio.to_thread(agent.invoke, {"input": request.query},
                                                     {"handle_parsing_errors": True})
                    response_text = result['output']
                    full_text = f"ğŸ¤– (æ•¸æ“šåˆ†æ): {response_text}"
                    for char in full_text:
                        yield char
                        await asyncio.sleep(0)
                    CHAT_HISTORY.append({"role": "User", "content": request.query})
                    CHAT_HISTORY.append({"role": "AI", "content": response_text})
                    return
                except Exception as e:
                    logger.warning(f"Agent åŸ·è¡Œå¤±æ•—: {e}")
                    total_rows = sum(len(df) for df in list(GLOBAL_DFS.values()))
                    if is_calc_query and total_rows > 20:
                        error_msg = f"âš ï¸ æŠ±æ­‰ï¼Œè¨ˆç®—å¼•æ“æš«æ™‚ç„¡æ³•è™•ç† (éŒ¯èª¤ä»£ç¢¼: OutputParserException)ã€‚\nå»ºè­°æ‚¨ï¼š\n1. å˜—è©¦æ›´æ˜ç¢ºçš„æŒ‡ä»¤\n2. æª¢æŸ¥æª”æ¡ˆå…§å®¹æ˜¯å¦ä¹¾æ·¨"
                        yield error_msg
                        CHAT_HISTORY.append({"role": "User", "content": request.query})
                        CHAT_HISTORY.append({"role": "AI", "content": error_msg})
                        return
                    logger.warning(f"åˆ‡æ›è‡³ RAG æ¨¡å¼å˜—è©¦å›ç­”...")

            # RAG æ··åˆæª¢ç´¢ (æ–‡å­—æ¨¡å¼)
            mode = "rag"
            # é€™è£¡ä¸éœ€è¦å†é‡æ–°æª¢ç´¢äº†ï¼Œä¸Šé¢å·²ç¶“ç”¢ç”Ÿ files_contextï¼Œä¸”æœƒåŒ…å« RAG çš„æ–‡å­—ç‰‡æ®µ
            # æˆ‘å€‘åªéœ€è¦åŠ å¼·æª¢ç´¢ï¼Œå¦‚æœä¸Šé¢çš„ç°¡æ˜“æª¢ç´¢ä¸å¤ 
            if len(GLOBAL_DOCS) > 0 and vector_db:
                try:
                    chroma_retriever = vector_db.as_retriever(search_kwargs={"k": 5})  # åŠ å¼·æª¢ç´¢æ·±åº¦
                    bm25_retriever = BM25Retriever.from_documents(GLOBAL_DOCS)
                    bm25_retriever.k = 5
                    ensemble_retriever = EnsembleRetriever(
                        retrievers=[chroma_retriever, bm25_retriever],
                        weights=[0.7, 0.3]
                    )
                    initial_docs = ensemble_retriever.invoke(request.query)
                    if initial_docs:
                        # é‡æ–°çµ„åˆæ›´è©³ç´°çš„ Context
                        rag_details = "\n\n".join(
                            [f"ã€ä¾†æºï¼š{d.metadata.get('source', 'æœªçŸ¥')}ã€‘\n{d.page_content}" for d in initial_docs])
                        files_context += f"\n\n=== è©³ç´°æª¢ç´¢å…§å®¹ ===\n{rag_details}"
                except Exception as e:
                    logger.error(f"æ·±åº¦æª¢ç´¢å¤±æ•—: {e}")

            template = f"""ç³»çµ±æŒ‡ä»¤:
            ä½ æ˜¯ä¸€ä½å°ˆæ¥­åŠ©ç†ã€‚ç›®å‰ç³»çµ±ä¸­å·²è¼‰å…¥çš„æª”æ¡ˆæ¸…å–®èˆ‡å…§å®¹æ‘˜è¦å¦‚ä¸‹ï¼š
            [{files_context}]

            è«‹å„ªå…ˆåƒè€ƒä¸Šè¿° [åƒè€ƒè³‡æ–™] å›ç­”å•é¡Œã€‚

            âš ï¸ å›ç­”è¦å‰‡ï¼š
            1. **å„ªå…ˆåƒè€ƒæ­·å²å°è©±**ï¼šè‹¥ [ç›®å‰å•é¡Œ] æåˆ°ã€Œå‰›æ‰ã€ã€ã€Œé‚£å€‹å…¬å¼ã€ï¼Œè«‹å¾ [æ­·å²å°è©±] æ‰¾å°‹ä¸Šä¸‹æ–‡ã€‚
            2. **ç›´æ¥å›ç­”å•é¡Œ**ï¼Œèªæ°£è‡ªç„¶ã€‚
            3. **ä»»å‹™å€åˆ†**ï¼š
                - **äº‹å¯¦æŸ¥è©¢**ï¼šå¼•ç”¨åƒè€ƒè³‡æ–™ã€‚
                - **ç¿»è­¯/ç¸½çµ**ï¼šè«‹ä½¿ç”¨ä½ çš„èªè¨€èƒ½åŠ›ç”Ÿæˆã€‚
            4. è‹¥æª”æ¡ˆä¸­åŒ…å«å¤šå€‹é¡Œç›®ï¼Œè«‹ä»”ç´°è¾¨åˆ¥ã€‚

            [æ­·å²å°è©±]: {{history}}
            [ç›®å‰å•é¡Œ]: {{question}}
            å›ç­”:"""

            prompt = ChatPromptTemplate.from_template(template)
            rag_chain = (
                    {"question": RunnablePassthrough(), "history": lambda x: history_str}
                    | prompt | llm | StrOutputParser()
            )

            full_response = ""
            async for chunk in rag_chain.astream(request.query):
                full_response += chunk
                yield chunk

            CHAT_HISTORY.append({"role": "User", "content": request.query})
            CHAT_HISTORY.append({"role": "AI", "content": full_response})

        return StreamingResponse(generate_response(), media_type="text/plain")

    except Exception as e:
        logger.error(f"Chat Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn

    print("ğŸš€ æ­£åœ¨å•Ÿå‹•å¾Œç«¯ API ä¼ºæœå™¨...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
