# app/services/cache_service.py
import time
import logging
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from app.core.config import settings

logger = logging.getLogger(__name__)


class SemanticCacheService:
    _instance = None

    def __init__(self):
        logger.info(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–èªæ„å¿«å–: {settings.CACHE_DB_DIR}")
        self.embeddings = HuggingFaceEmbeddings(model_name=settings.EMBEDDING_MODEL)

        # æ³¨æ„ï¼šé€™è£¡ç”¨ä¸åŒçš„ persist_directory å’Œ collection_name
        self.db = Chroma(
            collection_name="semantic_cache",
            persist_directory=settings.CACHE_DB_DIR,
            embedding_function=self.embeddings
        )
        self.threshold = 0.35  # ç›¸ä¼¼åº¦é–€æª»

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def check_cache(self, query: str):
        """æª¢æŸ¥æ˜¯å¦æœ‰å¿«å–"""
        start_time = time.time()
        results = self.db.similarity_search_with_score(query, k=1)

        if results:
            doc, score = results[0]
            if score < self.threshold:
                elapsed = time.time() - start_time
                logger.info(f"âš¡ [Cache Hit] å‘½ä¸­å¿«å–! (è·é›¢: {score:.4f} | è€—æ™‚: {elapsed:.4f}s)")
                return doc.page_content

        return None

    def update_cache(self, query: str, answer: str):
        """å¯«å…¥å¿«å–"""
        if not answer or len(answer) < 5: return

        self.db.add_documents([
            Document(page_content=answer, metadata={"question": query})
        ])
        logger.info(f"ğŸ’¾ [Cache Update] å·²å°‡å•ç­”å¯«å…¥å¿«å–")