import logging
import os
import time
from typing import AsyncGenerator
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from app.core.config import settings
from app.services.vector_store import VectorStoreService
from app.services.cache_service import SemanticCacheService
from app.utils.text_processor import TextProcessor

logger = logging.getLogger(__name__)


class ChatService:
    def __init__(self):
        # å¼·åˆ¶è¨­å®šä¸ä½¿ç”¨ä»£ç†
        os.environ["NO_PROXY"] = "*"
        os.environ["no_proxy"] = "*"

        self.vector_store = VectorStoreService.get_instance()
        self.cache = SemanticCacheService.get_instance()
        self.upload_dir = os.path.join(os.getcwd(), "uploads")

        # ğŸ”¥ğŸ”¥ğŸ”¥ å›æ­¸åŸé»ï¼šä½¿ç”¨æ‚¨åŸæœ¬é‹ä½œæ­£å¸¸çš„ gpt-oss:20b ğŸ”¥ğŸ”¥ğŸ”¥
        target_model = "gpt-oss:20b"

        logger.info(f"æ­£åœ¨åˆå§‹åŒ–èŠå¤©æ¨¡å‹: {target_model}")

        self.llm = ChatOllama(
            base_url=settings.OLLAMA_BASE_URL,
            model=target_model,
            temperature=0.1,  # ä¿æŒä½æº«ï¼Œè®“å›ç­”ç©©å®š
            keep_alive="1h",

            # âœ… åªä¿ç•™é€™å…©å€‹å¿…è¦çš„ RAG åƒæ•¸ï¼Œå…¶ä»–èŠ±ä¿çš„è¨­å®šé€šé€šç§»é™¤
            num_ctx=8192,  # ç¢ºä¿èƒ½è®€å–é•·ç¯‡ PDF
            num_predict=4096  # ç¢ºä¿è¡¨æ ¼èƒ½ç•«å®Œï¼Œä¸æœƒæ–·åœ¨ä¸­é–“
        )

    def _get_valid_files(self) -> list:
        """éæ¿¾æ‰éš±è—æª”èˆ‡æš«å­˜æª”"""
        if not os.path.exists(self.upload_dir):
            return []

        return [
            f for f in os.listdir(self.upload_dir)
            if os.path.isfile(os.path.join(self.upload_dir, f))
               and not f.startswith("~$")
               and f.lower() != "thumbs.db"
               and not f.endswith(".tmp")
        ]

    def _get_sorted_file_list(self, files: list) -> str:
        """ç”Ÿæˆæª”æ¡ˆæ¸…å–®å­—ä¸²"""
        if not files:
            return "(ç›®å‰è³‡æ–™åº«ç‚ºç©º)"

        try:
            file_info_list = []
            for f in files:
                file_path = os.path.join(self.upload_dir, f)
                mod_time = os.path.getmtime(file_path)
                file_info_list.append((f, mod_time))

            file_info_list.sort(key=lambda x: x[1], reverse=True)

            top_n = 10
            recent_files = file_info_list[:top_n]

            formatted_list = []
            for index, (fname, timestamp) in enumerate(recent_files):
                time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp)) if timestamp > 0 else "Unknown"

                icon = "ğŸ“„"
                if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):
                    icon = "ğŸ–¼ï¸"

                if index == 0:
                    formatted_list.append(f"- {icon} {fname} (âœ¨ NEWEST / æœ€æ–°ä¸Šå‚³) [Time: {time_str}]")
                else:
                    formatted_list.append(f"- {icon} {fname} [Time: {time_str}]")

            if len(file_info_list) > top_n:
                formatted_list.append(f"... (ä»¥åŠå…¶ä»– {len(file_info_list) - top_n} å€‹è¼ƒèˆŠçš„æª”æ¡ˆ)")

            return "\n".join(formatted_list)
        except Exception as e:
            logger.error(f"æ’åºå¤±æ•—: {e}")
            return "(ç„¡æ³•å–å¾—æª”æ¡ˆåˆ—è¡¨)"

    async def process_query(self, query: str, history: list) -> AsyncGenerator[str, None]:
        real_query = query

        valid_files = self._get_valid_files()
        has_files = len(valid_files) > 0

        if not has_files:
            yield "âš ï¸ ç›®å‰è³‡æ–™åº«æ˜¯ç©ºçš„ (0 å€‹æª”æ¡ˆ)ã€‚\n\nè«‹å…ˆä¸Šå‚³æ–‡ä»¶ã€‚"
            return

        file_count = len(valid_files)
        file_list_str = self._get_sorted_file_list(valid_files)

        # RAG æª¢ç´¢
        docs = self.vector_store.search(real_query, k=4)
        full_text = TextProcessor.smart_merge(docs)
        final_context = full_text if full_text else "æ²’æœ‰æª¢ç´¢åˆ°å…·é«”å…§å®¹ã€‚"

        history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-2:]]) if history else "(ç„¡æ­·å²ç´€éŒ„)"

        template_str = """You are a helpful AI assistant.

        ã€Filesã€‘:
        {file_list_str}

        ã€Contextã€‘:
        {context}

        ã€Historyã€‘:
        {history}

        ã€User Questionã€‘: {question}

        Instructions:
        1. Answer based on the Context.
        2. If the user asks for content, summarize it clearly.
        3. âœ… **Use Markdown Tables** for structured data.
        4. âŒ **Do NOT use HTML tags** (like <br>). Use standard Markdown newlines.
        5. Answer in Traditional Chinese (ç¹é«”ä¸­æ–‡).

        Answer:"""

        prompt = ChatPromptTemplate.from_template(template_str)

        chain = (
                {
                    "context": lambda x: final_context,
                    "question": RunnablePassthrough(),
                    "history": lambda x: history_text,
                    "file_list_str": lambda x: file_list_str,
                    "file_count": lambda x: str(file_count)
                }
                | prompt
                | self.llm
                | StrOutputParser()
        )

        try:
            async for chunk in chain.astream(real_query):
                # GPT-OSS é€šå¸¸æ ¼å¼æ¯”è¼ƒæ¨™æº–ï¼Œæˆ‘å€‘åªåšæœ€åŸºæœ¬çš„é˜²å‘†ï¼Œä¸åšéåº¦æ¸…æ´—
                clean_chunk = (chunk
                               .replace("<br>", "\n")
                               .replace("<br/>", "\n")
                               .replace("<b>", "**")
                               .replace("</b>", "**"))
                yield clean_chunk

        except Exception as e:
            logger.error(f"Chat Error: {e}")
            yield f"\n\nâš ï¸ ç™¼ç”ŸéŒ¯èª¤: {str(e)}\nè«‹æª¢æŸ¥é ç«¯ä¼ºæœå™¨é€£ç·šã€‚"