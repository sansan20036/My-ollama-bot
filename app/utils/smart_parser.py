# app/utils/smart_parser.py
import re
import unicodedata
from langchain_core.documents import Document

# ğŸ”¥ è¬èƒ½å¼•ç”¨å€å¡Š
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter, Language
except ImportError:
    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter, Language
    except ImportError:
        class Language:
            PYTHON = "python";
            JS = "js";
            TS = "ts";
            JAVA = "java";
            CPP = "cpp";
            GO = "go";
            HTML = "html"


        try:
            from langchain.text_splitter import RecursiveCharacterTextSplitter
        except ImportError:
            raise ImportError("âŒ åš´é‡éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° langchain æ–‡å­—åˆ‡å‰²æ¨¡çµ„ã€‚")


class SmartFileParser:
    """
    æ™ºæ…§æª”æ¡ˆè§£æå™¨ (Smart Routing Parser) V2.0
    ä¿®æ­£ï¼šè§£æ±ºæ³•å¾‹æ¢æ–‡å¼•ç”¨é€ æˆçš„éåº¦åˆ‡å‰²å•é¡Œ
    """

    def parse(self, raw_text: str, filename: str) -> list[Document]:
        text = self._clean_text(raw_text)
        documents = []

        if self._is_law_file(text):
            documents = self._parse_law(text, filename)
            doc_type = "æ³•å¾‹è¦ç« "
        elif self._is_code_file(filename):
            documents = self._parse_code(text, filename)
            doc_type = "ç¨‹å¼åŸå§‹ç¢¼"
        else:
            documents = self._parse_general(text, filename)
            doc_type = "ä¸€èˆ¬æ–‡ä»¶"

        # ç”Ÿæˆæ‘˜è¦å¡
        total_chars = len(text)
        total_chunks = len(documents)

        summary_content = (
            f"ã€æª”æ¡ˆå…¨åŸŸæ‘˜è¦ã€‘\n"
            f"æª”æ¡ˆåç¨±ï¼š{filename}\n"
            f"æ–‡ä»¶é¡å‹ï¼š{doc_type}\n"
            f"ç¸½å­—æ•¸ï¼šç´„ {total_chars} å­—\n"
            f"è³‡æ–™åˆ‡ç‰‡æ•¸ï¼š{total_chunks} å€‹æ®µè½\n"
        )

        if doc_type == "æ³•å¾‹è¦ç« ":
            article_count = len([d for d in documents if d.metadata.get("type") == "law_article"])
            summary_content += f"æ³•è¦æ¢æ–‡æ•¸ï¼šå…± {article_count} æ¢\n"

        summary_doc = Document(
            page_content=summary_content,
            metadata={
                "source": filename,
                "type": "file_summary",
                "total_chars": total_chars,
                "doc_type": doc_type
            }
        )
        documents.insert(0, summary_doc)
        return documents

    def _clean_text(self, text: str) -> str:
        # å¼·åˆ¶è½‰åŠå½¢ï¼Œä½†ä¿ç•™æ›è¡Œç¬¦è™Ÿï¼
        return unicodedata.normalize('NFKC', text)

    def _is_law_file(self, text: str) -> bool:
        return text.count("ç¬¬") > 5 and text.count("æ¢") > 5 and ("æ³•" in text or "æ¢ä¾‹" in text)

    def _is_code_file(self, filename: str) -> bool:
        if '.' not in filename: return False
        ext = filename.split('.')[-1].lower()
        return ext in ['py', 'js', 'ts', 'java', 'cpp', 'c', 'html', 'css', 'sql', 'go', 'rs']

    def _parse_law(self, text: str, filename: str) -> list[Document]:
        """
        ğŸ”¥ V2.0 ä¿®æ­£ç‰ˆï¼šåš´æ ¼åˆ‡å‰²
        åªæŠ“å–ä½æ–¼ã€Œè¡Œé¦–ã€æˆ–ã€Œæ›è¡Œå¾Œã€çš„ç¬¬Xæ¢ï¼Œé¿å…æŠ“åˆ°å…§æ–‡å¼•ç”¨çš„æ¢è™Ÿ
        """
        # Regex è§£é‡‹ï¼š
        # (?:\n|^)  -> éæ•ç²ç¾¤çµ„ï¼šå¿…é ˆæ˜¯å­—ä¸²é–‹é ­ (^) æˆ–æ˜¯æ›è¡Œç¬¦è™Ÿ (\n)
        # \s* -> å…è¨±å‰é¢æœ‰ä¸€äº›ç©ºç™½
        # (ç¬¬...æ¢) -> æ•ç²ç¾¤çµ„ï¼šé€™æ˜¯æˆ‘å€‘è¦æŠ“çš„æ¨™é¡Œ
        pattern = r'(?:\n|^)\s*(ç¬¬\s*[0-9\u4e00-\u9fa5\d\-ï¼]+\s*æ¢)'

        # ä½¿ç”¨ split åˆ‡å‰²
        parts = re.split(pattern, text)
        docs = []

        # è™•ç†å‰è¨€
        if parts[0].strip():
            docs.append(Document(page_content=f"ã€å‰è¨€ã€‘\n{parts[0].strip()}",
                                 metadata={"source": filename, "type": "law_preamble"}))

        # å› ç‚º regex åŠ äº†è¡Œé¦–åˆ¤æ–·ï¼Œsplit å‡ºä¾†çš„ list çµæ§‹æœƒç¨å¾®è®Šå‹•
        # parts[0] = å‰è¨€
        # parts[1] = æ¨™é¡Œ1 (e.g. ç¬¬1æ¢)
        # parts[2] = å…§å®¹1
        # parts[3] = æ¨™é¡Œ2 ...

        for i in range(1, len(parts), 2):
            if i + 1 >= len(parts): break
            header = parts[i].strip()
            content = parts[i + 1].strip()

            # å¦‚æœå…§å®¹å¤ªçŸ­ä¸”å¾Œé¢ç·Šæ¥è‘—å¦ä¸€å€‹æ¨™é¡Œï¼Œå¯èƒ½æ˜¯èª¤åˆ‡ï¼Œä½†åœ¨åš´æ ¼æ¨¡å¼ä¸‹æ©Ÿç‡é™ä½
            # æå–æ¢è™Ÿ ID
            try:
                article_id = header.replace("ç¬¬", "").replace("æ¢", "").strip()
            except:
                article_id = "unknown"

            # çµ„åˆå®Œæ•´æ¢æ–‡
            full_text = f"{header}\n{content}"

            docs.append(Document(
                page_content=full_text,
                metadata={"source": filename, "type": "law_article", "article_id": article_id}
            ))
        return docs

    def _parse_code(self, text: str, filename: str) -> list[Document]:
        # (ç¨‹å¼ç¢¼è§£æé‚è¼¯ä¿æŒä¸è®Š)
        ext = filename.split('.')[-1].lower()
        lang_map = {
            'py': Language.PYTHON, 'js': Language.JS, 'ts': Language.TS,
            'java': Language.JAVA, 'cpp': Language.CPP, 'c': Language.C,
            'html': Language.HTML, 'go': Language.GO
        }
        language = lang_map.get(ext, Language.PYTHON)
        try:
            splitter = RecursiveCharacterTextSplitter.from_language(
                language=language, chunk_size=800, chunk_overlap=200
            )
            raw_docs = splitter.create_documents([text])
            for d in raw_docs:
                d.metadata.update({"source": filename, "type": "code_snippet"})
            return raw_docs
        except Exception:
            return self._parse_general(text, filename)

    def _parse_general(self, text: str, filename: str) -> list[Document]:
        # (é€šç”¨è§£æé‚è¼¯ä¿æŒä¸è®Š)
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=100,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
        )
        chunks = splitter.split_text(text)
        return [Document(page_content=c, metadata={"source": filename, "type": "general_text"}) for c in chunks]