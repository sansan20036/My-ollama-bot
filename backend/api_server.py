import shutil
import os
import logging
import json
import gc
import pandas as pd
from typing import List
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse, StreamingResponse
from pydantic import BaseModel
import asyncio
import random
import requests

# ç¢ºä¿ file_factory å­˜åœ¨
from file_factory import FileLoaderFactory

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import CrossEncoder

# ğŸŸ¢ Import å€å¡Š
from langchain_community.retrievers import BM25Retriever

# ğŸŸ¢ æ‰‹å‹•å®šç¾© EnsembleRetriever (Polyfill)
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun


class EnsembleRetriever(BaseRetriever):
    """
    è‡ªå®šç¾©çš„æ··åˆæª¢ç´¢å™¨ (Polyfill)
    """
    retrievers: List[BaseRetriever]
    weights: List[float]

    def _get_relevant_documents(
            self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None
    ) -> List[Document]:
        all_docs = []
        seen_contents = set()
        for retriever in self.retrievers:
            try:
                docs = retriever.invoke(query)
                for doc in docs:
                    if doc.page_content not in seen_contents:
                        all_docs.append(doc)
                        seen_contents.add(doc.page_content)
            except Exception as e:
                print(f"âš ï¸ æª¢ç´¢å™¨åŸ·è¡ŒéŒ¯èª¤: {e}")
                continue
        return all_docs[:10]


# å¼•å…¥ Agent
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent

# è¨­å®š Log
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ================= é…ç½®å€ =================
MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)

DEFAULT_MODEL = "gpt-oss:20b"

# åˆå§‹åŒ–è®Šæ•¸
vector_db = None
CHAT_HISTORY = []
GLOBAL_FILE_CONTENT = ""
GLOBAL_DFS = []
GLOBAL_DOCS = []


def init_vector_db():
    global vector_db, GLOBAL_DOCS

    # ğŸŸ¢ 1. è¨­å®šè³‡æ–™åº«å„²å­˜è·¯å¾‘
    persist_dir = "./chroma_db"

    logger.info(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–è³‡æ–™åº«ï¼Œè·¯å¾‘: {persist_dir}")

    # ğŸŸ¢ 2. åˆå§‹åŒ– Chroma (æŒä¹…åŒ–)
    vector_db = Chroma(
        embedding_function=embeddings,
        persist_directory=persist_dir
    )

    # ğŸŸ¢ 3. é‡å»º BM25 ç´¢å¼•
    try:
        existing_data = vector_db.get()
        if existing_data and len(existing_data['ids']) > 0:
            print(f"ğŸ“¦ åµæ¸¬åˆ°æ­·å²å­˜æª”ï¼Œæ­£åœ¨é‡å»ºé—œéµå­—ç´¢å¼• (å…± {len(existing_data['ids'])} ç­†)...")

            GLOBAL_DOCS = []
            for i in range(len(existing_data['ids'])):
                doc = Document(
                    page_content=existing_data['documents'][i],
                    metadata=existing_data['metadatas'][i] if existing_data['metadatas'] else {}
                )
                GLOBAL_DOCS.append(doc)

            print(f"âœ… æˆåŠŸæ¢å¾©è¨˜æ†¶ï¼ç›®å‰æ“æœ‰ {len(GLOBAL_DOCS)} ç­†çŸ¥è­˜ç‰‡æ®µã€‚")
        else:
            print("âœ¨ å…¨æ–°é–‹å§‹ï¼šè³‡æ–™åº«ç›®å‰æ˜¯ç©ºçš„ã€‚")

    except Exception as e:
        print(f"âš ï¸ é‡å»ºç´¢å¼•æ™‚ç™¼ç”Ÿå°æ’æ›²: {e}")

    logger.info(f"âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")


init_vector_db()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)


class ChatRequest(BaseModel):
    query: str
    model_name: str = DEFAULT_MODEL


@app.get("/")
async def root():
    return RedirectResponse(url="/docs")


# ================= é‡ç½®é‚è¼¯ (Deep Clean) =================
@app.post("/reset")
async def reset_history():
    global CHAT_HISTORY, vector_db, GLOBAL_FILE_CONTENT, GLOBAL_DFS, GLOBAL_DOCS
    logger.info("ğŸ§¹ åŸ·è¡Œç³»çµ±é‡ç½® (Deep Clean)...")

    try:
        CHAT_HISTORY = []
        GLOBAL_FILE_CONTENT = ""
        GLOBAL_DFS = []
        GLOBAL_DOCS = []

        persist_dir = "./chroma_db"

        if vector_db:
            try:
                ids = vector_db.get()['ids']
                if ids:
                    vector_db.delete(ids)
            except Exception as e:
                logger.warning(f"é‚è¼¯åˆªé™¤å¤±æ•—: {e}")

            vector_db = None
            del vector_db
            gc.collect()

        if os.path.exists(persist_dir):
            try:
                await asyncio.sleep(0.5)
                shutil.rmtree(persist_dir, ignore_errors=True)
                logger.info("ğŸ—‘ï¸ å·²åˆªé™¤å¯¦é«”è³‡æ–™åº«æª”æ¡ˆ")
            except Exception as e:
                logger.error(f"âŒ ç‰©ç†åˆªé™¤å¤±æ•—: {e}")

        init_vector_db()
        return {"message": "ç³»çµ±å·²å®Œå…¨é‡ç½® (è¨˜æ†¶å·²æ¸…é™¤)"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ================= ç²å–æ¨¡å‹æ¸…å–® API =================
@app.get("/models")
async def get_models():
    try:
        ollama_api_url = "http://git.tedpc.com.tw:11434/api/tags"
        response = requests.get(ollama_api_url, timeout=5)
        response.raise_for_status()
        data = response.json()
        models = [model["name"] for model in data.get("models", [])]
        return {"models": models}
    except Exception as e:
        logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹æ¸…å–®: {e}")
        return {"models": ["gpt-oss:20b", "llama3.1:latest"]}


# ================= ä¸Šå‚³é‚è¼¯ =================
@app.post("/upload")
async def upload_files(files: List[UploadFile] = File(...)):
    #await reset_history()
    global vector_db, GLOBAL_FILE_CONTENT, GLOBAL_DFS, GLOBAL_DOCS

    try:
        processed_files = []
        full_text_list = []
        temp_dfs = []

        for file in files:
            temp_filename = f"temp_{file.filename}"
            with open(temp_filename, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)

            if file.filename.endswith(('.xlsx', '.xls')):
                try:
                    df = pd.read_excel(temp_filename, engine='openpyxl')
                    temp_dfs.append(df)
                except Exception:
                    pass
            elif file.filename.endswith('.csv'):
                try:
                    df = pd.read_csv(temp_filename)
                    temp_dfs.append(df)
                except Exception:
                    pass

            try:
                loader = FileLoaderFactory.get_loader(temp_filename, file.filename)
                raw_text = loader.extract_text()
                if raw_text:
                    full_text_list.append(f"ã€æª”æ¡ˆ: {file.filename}ã€‘\n{raw_text}\n")

                    chunks = text_splitter.split_text(raw_text)
                    docs = [Document(page_content=c, metadata={"source": file.filename}) for c in chunks]
                    if docs:
                        vector_db.add_documents(docs)
                        GLOBAL_DOCS.extend(docs)
                        processed_files.append(file.filename)
            finally:
                if os.path.exists(temp_filename):
                    os.remove(temp_filename)

        GLOBAL_DFS = temp_dfs
        combined_text = "\n".join(full_text_list)
        if len(combined_text) < 10000:
            GLOBAL_FILE_CONTENT = combined_text
        else:
            GLOBAL_FILE_CONTENT = ""

        mode = "RAG_MODE"
        if len(GLOBAL_DFS) > 0:
            mode = "PANDAS_AGENT"
        elif GLOBAL_FILE_CONTENT:
            mode = "GOD_MODE"

        return {"status": "success", "processed_files": processed_files, "mode": mode}

    except Exception as e:
        logger.error(f"ä¸Šå‚³å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ================= èŠå¤©é‚è¼¯ (ä¹¾æ·¨ç‰ˆ - ç„¡æ€è€ƒéç¨‹) =================
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    global CHAT_HISTORY, GLOBAL_FILE_CONTENT, GLOBAL_DFS, GLOBAL_DOCS

    logger.info(f"ğŸ—£ï¸ æ”¶åˆ°å•é¡Œ: {request.query}")

    try:
        model_to_use = request.model_name if request.model_name else DEFAULT_MODEL

        llm = ChatOllama(
            model=model_to_use,
            temperature=0.1,
            base_url="http://git.tedpc.com.tw:11434",
            request_timeout=500.0,
            headers={"X-Accel-Buffering": "no", "Cache-Control": "no-cache", "Connection": "keep-alive"}
        )

        real_query = request.query
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in CHAT_HISTORY[-4:]])

        # ğŸŸ¢ æº–å‚™å›å‚³çš„ç”Ÿæˆå™¨
        async def generate_response():
            final_query = real_query

            # ç­–ç•¥ 1: Pandas Agent
            if len(GLOBAL_DFS) > 0:
                try:
                    agent = create_pandas_dataframe_agent(
                        llm, GLOBAL_DFS, verbose=True, allow_dangerous_code=True,
                        agent_type="zero-shot-react-description", handle_parsing_errors=True
                    )
                    result = agent.invoke(f"è«‹æ ¹æ“šè³‡æ–™è¡¨å›ç­”ã€‚å•é¡Œ: {final_query}")
                    response_text = result['output']

                    # æ¨¡æ“¬æ‰“å­—æ©Ÿ (åƒ…é‡å° Agent)
                    full_text = f"ğŸ¤– (æ•¸æ“šåˆ†æ): {response_text}"
                    for char in full_text:
                        yield char
                        await asyncio.sleep(random.uniform(0.01, 0.05))

                    CHAT_HISTORY.append({"role": "User", "content": request.query})
                    CHAT_HISTORY.append({"role": "AI", "content": response_text})
                    return
                except Exception:
                    pass

            # ç­–ç•¥ 2: RAG æ··åˆæª¢ç´¢
            final_context = ""
            mode = "chitchat"

            if len(GLOBAL_DOCS) > 0 and vector_db:
                print(f"ğŸš€ å•Ÿå‹•æ··åˆæª¢ç´¢ (Hybrid Search): {real_query}")
                try:
                    chroma_retriever = vector_db.as_retriever(search_kwargs={"k": 5})
                    bm25_retriever = BM25Retriever.from_documents(GLOBAL_DOCS)
                    bm25_retriever.k = 5

                    ensemble_retriever = EnsembleRetriever(
                        retrievers=[chroma_retriever, bm25_retriever],
                        weights=[0.5, 0.5]
                    )
                    initial_docs = ensemble_retriever.invoke(final_query)

                    if initial_docs:
                        mode = "rag"
                        context_pieces = []
                        for doc in initial_docs:
                            source_name = doc.metadata.get("source", "æœªçŸ¥ä¾†æº")
                            piece = f"ã€ä¾†æºï¼š{source_name}ã€‘\n{doc.page_content}"
                            context_pieces.append(piece)
                        final_context = "\n\n".join(context_pieces)
                        print(f"âœ… æ··åˆæª¢ç´¢å‘½ä¸­ï¼åƒè€ƒäº† {len(initial_docs)} å€‹ç‰‡æ®µã€‚")
                except Exception as e:
                    print(f"Error: {e}")
                    mode = "chitchat"

            # 3. ç”Ÿæˆå›ç­”
            if mode == "rag":
                template = """ç³»çµ±æŒ‡ä»¤:
                            ä½ æ˜¯ä¸€ä½å°ˆæ¥­åŠ©ç†ã€‚è«‹æ ¹æ“šä¸‹æ–¹çš„ [åƒè€ƒè³‡æ–™] èˆ‡ [æ­·å²å°è©±] å›ç­” [ç›®å‰å•é¡Œ]ã€‚
                            âš ï¸ å›ç­”è¦å‰‡ï¼š
                            1. **ç›´æ¥å›ç­”å•é¡Œ**ï¼Œèªæ°£è‡ªç„¶ã€æœ‰ç¦®è²Œã€‚
                            2. ç­”æ¡ˆå¿…é ˆä¾†è‡ªåƒè€ƒè³‡æ–™ï¼Œä¸å¯çæ°ã€‚
                            3. å¦‚æœæ˜¯è¡¨æ ¼æ•¸æ“šï¼Œè«‹æ•´ç†æˆ Markdown è¡¨æ ¼ã€‚
                            4. å¦‚æœéœ€è¦è¨ˆç®—ï¼Œè«‹åˆ—å‡ºç®—å¼ã€‚
                            5. **éå¸¸é‡è¦ï¼š** åœ¨å›ç­”çš„æ¯ä¸€é»å¾Œé¢ï¼Œè«‹ç”¨æ‹¬è™Ÿæ¨™è¨»ä¾†æºæª”åï¼Œä¾‹å¦‚ï¼š(å‡ºè™•: å‹åŸºæ³•.pdf)ã€‚
                            6. **æ ¼å¼æ’ç‰ˆ**ï¼š
                               - æ¢åˆ—å¼é‡é»è«‹ç”¨ Markdown æ¸…å–®ã€‚
                               - æ•¸æ“šè«‹æ•´ç†æˆ Markdown è¡¨æ ¼ã€‚
                               - æ•¸å­¸å…¬å¼è«‹ç”¨ LaTeX æ ¼å¼ (ä¾‹å¦‚ $E=mc^2$)ã€‚

                            [åƒè€ƒè³‡æ–™]: {context}
                            [æ­·å²å°è©±]: {history}
                            [ç›®å‰å•é¡Œ]: {question}
                            å›ç­”:"""
            else:
                template = """ç³»çµ±æŒ‡ä»¤:
                ä½ æ˜¯ä¸€ä½å‹å–„ä¸”åšå­¸çš„ AI åŠ©ç†ã€‚æ ¹æ“š [æ­·å²å°è©±] å›ç­” [ç›®å‰å•é¡Œ]ã€‚
                [æ­·å²å°è©±]: {history}
                [ç›®å‰å•é¡Œ]: {question}
                å›ç­”:"""

            prompt = ChatPromptTemplate.from_template(template)
            rag_chain = (
                    {"context": lambda x: final_context, "question": RunnablePassthrough(),
                     "history": lambda x: history_str}
                    | prompt | llm | StrOutputParser()
            )

            full_response = ""
            async for chunk in rag_chain.astream(final_query):
                full_response += chunk
                yield chunk
                await asyncio.sleep(0)  # è®“å‡ºè³‡æºï¼Œä½†ä¸åˆ»æ„å»¶é²

            CHAT_HISTORY.append({"role": "User", "content": request.query})
            CHAT_HISTORY.append({"role": "AI", "content": full_response})

        return StreamingResponse(generate_response(), media_type="text/plain")

    except Exception as e:
        logger.error(f"Chat Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)