import shutil
import os
import logging
import json
from typing import List
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse, StreamingResponse
from pydantic import BaseModel

# å¼•ç”¨ä½ çš„æª”æ¡ˆè®€å–å·¥å» 
from file_factory import FileLoaderFactory

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

logger.info("æ­£åœ¨è¼‰å…¥ Embedding æ¨¡å‹...")
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

VECTOR_DB_PATH = "./chroma_db_api"


def get_vector_db():
    return Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)


vector_db = get_vector_db()
# åŸæœ¬å¯èƒ½æ˜¯ 500
text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)

CHAT_HISTORY = []


class ChatRequest(BaseModel):
    query: str
    model_name: str = "llama3.2"


@app.get("/")
async def root():
    return RedirectResponse(url="/docs")


@app.post("/reset")
async def reset_history():
    global CHAT_HISTORY, vector_db
    try:
        CHAT_HISTORY = []
        existing_data = vector_db.get()
        existing_ids = existing_data['ids']
        if existing_ids:
            vector_db.delete(existing_ids)
            logger.info(f"å·²æ¸…ç©ºè³‡æ–™åº«ï¼Œå…±åˆªé™¤ {len(existing_ids)} ç­†è³‡æ–™")
        return {"message": "ç³»çµ±å·²å®Œå…¨é‡ç½®"}
    except Exception as e:
        logger.error(f"Reset Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/upload")
async def upload_files(files: List[UploadFile] = File(...)):
    global CHAT_HISTORY, vector_db
    CHAT_HISTORY = []

    try:
        existing_ids = vector_db.get()['ids']
        if existing_ids:
            vector_db.delete(existing_ids)

        total_chunks = 0
        processed_files = []
        error_files = []

        for file in files:
            temp_filename = f"temp_{file.filename}"
            try:
                with open(temp_filename, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)

                logger.info(f"æ­£åœ¨è™•ç†: {file.filename}")
                loader = FileLoaderFactory.get_loader(temp_filename, file.filename)
                raw_text = loader.extract_text()

                if not raw_text or len(raw_text.strip()) == 0:
                    logger.warning(f"{file.filename} å…§å®¹ç‚ºç©ºï¼Œè·³é")
                    error_files.append(file.filename)
                    continue

                chunks_text = text_splitter.split_text(raw_text)
                documents = [
                    Document(page_content=chunk, metadata={"source": file.filename})
                    for chunk in chunks_text
                ]

                if documents:
                    vector_db.add_documents(documents)
                    total_chunks += len(documents)
                    processed_files.append(file.filename)

            except Exception as e:
                logger.error(f"è™•ç† {file.filename} å¤±æ•—: {str(e)}")
                error_files.append(f"{file.filename} ({str(e)})")

            finally:
                try:
                    if os.path.exists(temp_filename):
                        os.remove(temp_filename)
                except Exception as cleanup_error:
                    logger.warning(f"æš«å­˜æª”åˆªé™¤å¤±æ•— (å¿½ç•¥): {str(cleanup_error)}")

        return {
            "status": "success",
            "processed_files": processed_files,
            "message": f"æˆåŠŸè®€å– {len(processed_files)} å€‹æª”æ¡ˆ"
        }

    except Exception as e:
        logger.error(f"æ‰¹æ¬¡ä¸Šå‚³éŒ¯èª¤: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ğŸŸ¢ ä¿®æ­£é‡é»ï¼šåŠ å…¥ã€Œå°è©±æ”¹å¯«æ©Ÿåˆ¶ã€ä»¥æ”¯æ´å¤šè¼ªå°è©±
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    global CHAT_HISTORY
    try:
        llm = ChatOllama(
            model=request.model_name,
            temperature=0.1,
            base_url="http://localhost:11434",
            request_timeout=180.0
        )

        retriever = vector_db.as_retriever(search_kwargs={"k": 10})

        # å°‡æ­·å²ç´€éŒ„è½‰æˆå­—ä¸²
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in CHAT_HISTORY])


        # å¦‚æœæœ‰æ­·å²å°è©±ï¼Œå…ˆè«‹ AI æŠŠä½¿ç”¨è€…çš„å•é¡Œæ”¹å¯«æˆã€ŒåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡çš„å•å¥ã€
        real_query = request.query

        if CHAT_HISTORY:
            rephrase_prompt = ChatPromptTemplate.from_template("""
            Given the following conversation history and a follow-up question, 
            rephrase the follow-up question to be a standalone question that includes the necessary context.
            Do NOT answer the question, just rephrase it.

            Chat History:
            {history}

            Follow Up Input: {question}

            Standalone Question:""")

            rephrase_chain = rephrase_prompt | llm | StrOutputParser()

            # å–å¾—æ”¹å¯«å¾Œçš„æœå°‹é—œéµå­—
            real_query = rephrase_chain.invoke({"history": history_str, "question": request.query})
            logger.info(f"åŸå§‹å•é¡Œ: {request.query} -> æ”¹å¯«å¾Œæœå°‹: {real_query}")



        docs = retriever.invoke(real_query)

        # æº–å‚™ Sources (å›å‚³çµ¦å‰ç«¯é¡¯ç¤ºç”¨)
        sources = list(set([doc.metadata.get("source", "æœªçŸ¥") for doc in docs]))
        sources_json = json.dumps(sources, ensure_ascii=True)


        # é€™è£¡çš„ Prompt å¯ä»¥ç¶­æŒåŸæ¨£ï¼Œæˆ–æ˜¯å¼·èª¿åƒè€ƒ Context
        user_query_lower = request.query.lower()
        force_english = "english" in user_query_lower or "è‹±æ–‡" in user_query_lower

        if force_english:
            template = """You are a helpful AI assistant.
            Answer the user's question based on the Context below.
            If the answer is not in the context, say "I don't have that information."

            Context:
            {context}

            History:
            {history}

            User Question: {question}

            Answer:"""
        else:
            template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­åŠ©ç†ã€‚
            è«‹æ ¹æ“šä¸‹æ–¹çš„ã€Œå·²çŸ¥è³‡è¨Šã€èˆ‡ã€Œæ­·å²å°è©±ã€ä¾†å›ç­”å•é¡Œã€‚

            é‡è¦ï¼š
            1. å¦‚æœé€™å€‹å•é¡Œæ˜¯æ‰¿æ¥ä¸Šä¸€å¥çš„(ä¾‹å¦‚ã€Œé‚£ç¼ºè²¨çš„æ˜¯å“ªå€‹ï¼Ÿã€)ï¼Œè«‹å‹™å¿…çµåˆæ­·å²å°è©±ä¾†ç†è§£ã€‚
            2. å¦‚æœè³‡æ–™åº«çœŸçš„æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹ç›´èªªã€‚

            å·²çŸ¥è³‡è¨Š:
            {context}

            æ­·å²å°è©±:
            {history}

            ä½¿ç”¨è€…å•é¡Œ: {question}

            å›ç­”:"""

        prompt = ChatPromptTemplate.from_template(template)

        rag_chain = (
                {
                    "context": lambda x: docs,  # ç›´æ¥ä½¿ç”¨å‰›å‰›æª¢ç´¢åˆ°çš„ docs
                    "question": RunnablePassthrough(),
                    "history": lambda x: history_str
                }
                | prompt
                | llm
                | StrOutputParser()
        )

        async def generate_response():
            full_response = ""
            # é€™è£¡æˆ‘å€‘å‚³å…¥åŸå§‹å•é¡Œ request.query çµ¦ LLM ç”Ÿæˆå›ç­”ï¼Œå› ç‚º Context å·²ç¶“æŠ“å°äº†
            async for chunk in rag_chain.astream(request.query):
                full_response += chunk
                yield chunk

            # å­˜å…¥æ­·å²ç´€éŒ„
            CHAT_HISTORY.append({"role": "User", "content": request.query})
            CHAT_HISTORY.append({"role": "AI", "content": full_response})

        return StreamingResponse(
            generate_response(),
            media_type="text/plain",
            headers={"X-Sources": sources_json}
        )

    except Exception as e:
        logger.error(f"Chat Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))